{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a6e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65268ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/Odyssey/public/glorys/reanalysis/glorys12_2010_2019_daily_zos.nc', chunks=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c22024ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function gc.collect(generation=2)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_list = [\"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\"]\n",
    "adt_list =[]\n",
    "for i in year_list :\n",
    "    adt_list.append(ds.rename({\"zos\" : \"adt\"}).sel(time = i))\n",
    "    \n",
    "del ds\n",
    "gc.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58da9568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeAnomaly(input_ds : xr.Dataset) -> xr.Dataset :\n",
    "    \n",
    "    mean = input_ds.groupby(\"time.month\").mean(dim=\"time\")\n",
    "    anomaly = (input_ds.groupby(\"time.month\") - mean).rename({\"adt\" : \"sla\"}).load()\n",
    "    del mean\n",
    "    gc.collect\n",
    "    \n",
    "    return anomaly\n",
    "\n",
    "\n",
    "def interpolate(input_ds : xr.Dataset) -> xr.Dataset :\n",
    "    \n",
    "    # for i in range (0,10) :\n",
    "        \n",
    "    #     chunck_size = input_ds.coords[\"time\"].size \n",
    "    #     processing = input_ds.isel(time = slice(int(i / 10 * chunck_size), int((i + 1) / 10 * chunck_size)))\n",
    "    #     chunck_interp = (processing.interp(\n",
    "    #                                 coords=dict(\n",
    "    #                                     latitude=np.arange(input_ds.latitude[0], input_ds.latitude[-1], float(1/8)),\n",
    "    #                                     longitude=np.arange(input_ds.longitude[0], input_ds.longitude[-1], float(1/8))\n",
    "    #                                 ),\n",
    "    #                                 method=\"linear\"\n",
    "    #                             )\n",
    "    #                     )\n",
    "        \n",
    "    #     interp_list.append(chunck_interp)\n",
    "    #     del chunck_interp, processing\n",
    "    #     gc.collect\n",
    "\n",
    "    new_lat = np.arange(input_ds.latitude[0], input_ds.latitude[-1], float(1/8))\n",
    "    new_lon = np.arange(input_ds.longitude[0], input_ds.longitude[-1], float(1/8))\n",
    "    \n",
    "    chunked = input_ds.chunk(365)    \n",
    "    lat_interped = chunked.interp(latitude=new_lat, method=\"linear\", assume_sorted=True)\n",
    "    interpolated = lat_interped.interp(longitude = new_lon, method= \"linear\", assume_sorted=True)\n",
    "\n",
    "                \n",
    "    return interpolated.load()\n",
    "        \n",
    "def getNorm(input_ds : xr.Dataset) -> list[float] :\n",
    "    chunked = input_ds.chunk(365)\n",
    "    \n",
    "    n = chunked.count()\n",
    "    sum = chunked.sum(skipna=True)\n",
    "    mu = (sum / n).load()\n",
    "    var = ((chunked - mu)**2).sum(skipna=True) / n\n",
    "    sigma = var.load()**0.5\n",
    "    \n",
    "    out_list = [mu, sigma]\n",
    "    \n",
    "    del n, sum, mu, var, sigma\n",
    "    gc.collect\n",
    "    \n",
    "    return out_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6831591",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_list = []\n",
    "\n",
    "for i in adt_list :\n",
    "\n",
    "    a = makeAnomaly(i)\n",
    "    b = interpolate(a)\n",
    "    processed_list.append(b)\n",
    "    \n",
    "    del a, b\n",
    "    gc.collect\n",
    "    \n",
    "del adt_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ebc22e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function gc.collect(generation=2)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_ds = xr.concat(processed_list, dim=\"time\").drop_vars(\"month\")\n",
    "del processed_list\n",
    "gc.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "407c46d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ds.to_netcdf(\"/Odyssey/public/glorys/reanalysis/glorys12_2010_2019_daily_sla_8th_test.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0632e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "start = processed_ds.time[0].dt.date\n",
    "last = processed_ds.time[-1].dt.date\n",
    "train_ds = processed_ds.sel(time = slice(start, last-timedelta(days=365)))\n",
    "valid_ds = processed_ds.sel(time = slice(last-timedelta(days=365), last))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "065d881f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function gc.collect(generation=2)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del processed_ds\n",
    "gc.collect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f77c7b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training normalizer : [<xarray.Dataset> Size: 8B\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    sla      float64 8B -2.073e-20, <xarray.Dataset> Size: 8B\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    sla      float64 8B 0.04391]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Training normalizer : {getNorm(train_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0aa178e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation normalizer : [<xarray.Dataset> Size: 8B\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    sla      float64 8B -3.198e-06, <xarray.Dataset> Size: 8B\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    sla      float64 8B 0.04427]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Validation normalizer : {getNorm(valid_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aacc881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "varn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
